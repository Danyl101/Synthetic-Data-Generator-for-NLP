# ðŸ“ Semi-Synthetic Data Generator for NLP

This project is a **semi-synthetic dataset generator for NLP tasks**, designed to scrape  news, extract article content, paraphrase it using state-of-the-art models, and filter the results for quality.

It was built as a modular pipeline combining **web scraping, article extraction, paraphrasing, semantic filtering, and API/React integration**, to create robust and diverse datasets for downstream machine learning models like **SBERT**.

---

## ðŸš€ Features

* **Scraper Module**

  * Scrapes financial/economic news sites using **Selenium**.
  * Handles static and dynamic pages.
  * Supports blacklist filtering, error handling, and modular configs via JSON.

* **Extractor Module**

  * Extracts full article content using **newspaper3k**, Selenium, and Playwright fallback.
  * Cleans malformed HTML and JS-heavy sites.
  * Stores articles as clean `.txt` files.

* **Paraphraser Module**

  * Uses **Pegasus** for high-quality paraphrasing.
  * Handles hierarchical file structures with robust dictionary-based storage.
  * Outputs paraphrased versions of extracted text files.

* **SBERT Semantic Filter**

  * Compares paraphrased text with originals using **Sentence-BERT**.
  * Removes files with too high (>0.9) or too low (<0.3) similarity.
  * Applies heuristics to handle chunking/empty embeddings.

* **API Module**

  * Built with **Flask (Python)** and **TypeScript**.
  * Provides endpoints for:
    * Managing filters and site lists
    * Running scraper & extractor
    * Running Paraphrasing and semantic filter
  * Async operations with logging for reliability.

* **Frontend**

  * Built with **React + Vite + Tailwind + Shadcn**.
  * Provides simple UI for interacting with APIs.
  * Clean minimal design suitable for demos.

---

## ðŸ› ï¸ Tech Stack

* **Backend**: Python (Flask, Selenium, Playwright, Newspaper3k, Pegasus, SBERT, PyTorch)
* **Frontend**: React (Vite, TailwindCSS, Shadcn UI)
* **ML Models**: Pegasus (Paraphrasing), SBERT (Semantic Similarity), LSTM/TCN/BiLSTM (Forecasting)
* **Infrastructure**: JSON-config driven, modular architecture

---

## ðŸ“‚ Project Structure

```
.
â”œâ”€â”€ scraper/               # Web scraping modules
â”‚   â”œâ”€â”€ link_extract.py
â”‚   â”œâ”€â”€ robot.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ scrape_run.py
â”‚
â”œâ”€â”€ extractor/             # Article extraction modules
â”‚   â”œâ”€â”€ selenium_newspaper.py
â”‚   â”œâ”€â”€ playwright_extract.py
â”‚   â”œâ”€â”€ content_extract.py
â”‚   â””â”€â”€ extract_run.py            
â”‚
â”œâ”€â”€ frontend/              # React + Vite frontend
â”‚    â””â”€â”€src/
|        â”œâ”€â”€API/           # Flask APIs
|        â””â”€â”€UI/
|
â”œâ”€â”€ Datasets/              # Scraped & processed datasets
|    â”œâ”€â”€BERT_Data/
|    |    â””â”€â”€ Raw_Text_Data/
|    |          â”œâ”€â”€ Raw_Data
|    |          â”œâ”€â”€ Cleaned_Data
|    |          â””â”€â”€ Paraphrased_Data
|    |
|    â””â”€â”€Scraping_Data/
|         â”œâ”€â”€ Scraped_articles.json
|         â””â”€â”€Site_filters.json
|
â””â”€â”€BERT_Preprocess/
      â”œâ”€â”€ Clean_Run.py
      â”œâ”€â”€ Paraphraser.py     #Paraphraser
      â”œâ”€â”€ Bert_Semantic.py   #SBERT Comparison
      â””â”€â”€ Preprocess_run.py
```

---

## âš™ï¸ Installation

Clone the repo:

```bash
git clone https://github.com/Danyl101/Synthetic-Data-Generator-for-NLP.git
```

Install backend dependencies:

```bash
pip install -r requirements.txt
```

Install frontend dependencies:

```bash
cd frontend
npm install
```

---

## â–¶ï¸ Usage

1. **Start Flask API**

   ```bash
   $env:PYTHONPATH = (Get-Location).Path
   python -m frontend.src.api.flask_api.flask_run
   ```

2. **Run Frontend**

   ```bash
   cd frontend
   npm run dev
   ```

---

## ðŸ” Iterative Development

This system evolved through multiple iterations:

* **Scraper**: Static â†’ Selenium â†’ Full dynamic scraping with error handling.
* **Extractor**: Newspaper3k â†’ XPath fixes â†’ Playwright fallback.
* **Paraphrase**: Backtranslation â†’ Pegasus with chunk handling.
* **SBERT**: Per-chunk scores â†’ Averaged per-file â†’ Heuristic cleanup.
* **API**: Simple JSON endpoints â†’ Modular Flask services â†’ ML inference integration.
* **React**: Barebones â†’ Functional â†’ Presentable with Tailwind + Shadcn.

---

## ðŸ“Œ Future Improvements

* Add caching for scraper/extractor.
* Parallelize Playwright for speed.
* More paraphrasing diversity (mix Pegasus + LLM).
* Add dataset export formats (CSV, JSONL, HuggingFace Datasets).
* Containerize with Docker for deployment.

---

## ðŸ“œ License

MIT License. See `LICENSE` for details.

---

Would you like me to make this **research-style (academic tone, like a paper)** or **developer-style (GitHub OSS tone, more practical and concise)**?
